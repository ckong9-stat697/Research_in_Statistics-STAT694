---
title: "STAT694_Project_Analysis"
author: "Chun Yin Kong"
date: "10/5/2020"
output: rmarkdown::github_document
editor_options: 
  chunk_output_type: inline
---

\begin{center}
\textbf{A Research on the relationship between the seriousness of Natural Disaster and the Sentiment of Twitter Text - A COVID-19 Pandamic Case Study}
\end{center}
### Introduction

#### Research Purpose

In this research project, I am trying to analyze the effect of natural disaster to the sentiment level of the text. We know that in 2020, COVID-19 has affected the whole world and we experienced different levels of community, cities, or even country lockdown, limiting our social activities, teaching and working life, sickness and deaths. As we spent more time on internet, Twitter, one of the very popular social media platform in the US, has tons of texts and information update daily. I would like to through learning the tweets and the geo-location of tweets, to study the effect of natural disaster to our feeling.


#### References to this Project Analysis:

##### An Introduction to Cleaning of Text:
https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf

##### Tidy Text Mining Method in R:
https://www.tidytextmining.com/twitter.html

##### Obtaining Population Details from US Census:
https://www.census.gov/data/academy/courses/ranking-project.html

#### Methodology

I first obtain real tweets from Twitter daily, from Oct 5 to Oct X, 2020. In the process of scraping tweets, I used Twitter API in Python, which the relevant codes can be found in the second page of the research report as well. Then, the downloaded data set will be in csv format which are able to load into R for the analysis. Hence, in the following pages the code and result will demonstrate the process and the final analysis.

\newpage

### Data Scraping using Twitter APIs on Python:
In the data scraping stage, I used Python as my programming language because I am more familiar with Python in APIs. First I connected my codes to my free Twitter developer account using my own credentials and tokens. Hence I set some filtering parameters so that the exported csv data set is the result that I would like to see. For privacy and security issue, I hidden my credentials and tokens manually.

Tweepy Library is one of the Python API that is easy to use to connect to Twitter and Scrap the required tweets, location, time of tweet creations, username. 

Since I only analyze the tweets with location and perform sentiment analysis, I do not need to include usernames, userID, and various other information that will reveal individual user's identification. In the code stage I already do not include the extraction of user information.

```{python eval=FALSE}
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import tweepy
import json
import pandas as pd
import csv
import re
from textblob import TextBlob
import string
import preprocessor as p
import os
import time
```


```{python eval=FALSE}
# Twitter credentials
# Obtain them from your twitter developer account
consumer_key = "aaaaaaaaaaaaaaaaaaaaaaaaaa"
consumer_secret = "bbbbbbbbbbbbbbbbbbbbbbbbbbbbbb"
access_key = "ccccccccccccccccccccccccccccccccc"
access_secret = "ddddddddddddddddddddddddddddddd"
# Pass your twitter credentials to tweepy via its OAuthHandler

auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_key, access_secret)
api = tweepy.API(auth)
```

```{python eval=FALSE}
def scraptweets(search_words, date_since, numTweets, numRuns):
    
    # Define a for-loop to generate tweets at regular intervals
    # We cannot make large API call in one go. Hence, let's try T times
    
    # Define a pandas dataframe to store the date:
    db_tweets = pd.DataFrame(columns = ['location', 'text', 'hashtags', 'tweetcreatedts'])
    program_start = time.time()
    for i in range(0, numRuns):
        # We will time how long it takes to scrape tweets for each run:
        start_run = time.time()
        
        # Collect tweets using the Cursor object
        # .Cursor() returns an object that you can iterate or loop over to access the data collected.
        # Each item in the iterator has various attributes that you can access to get information about each tweet
        tweets = tweepy.Cursor(api.search, q=search_words, lang="en", since=date_since, tweet_mode='extended').items(numTweets)# Store these tweets into a python list
        tweet_list = [tweet for tweet in tweets]# Obtain the following info (methods to call them out):
        # user.screen_name - twitter handle
        # user.description - description of account
        # user.location - where is he tweeting from
        # user.friends_count - no. of other users that user is following (following)
        # user.followers_count - no. of other users who are following this user (followers)
        # user.statuses_count - total tweets by user
        # user.created_at - when the user account was created
        # created_at - when the tweet was created
        # retweet_count - no. of retweets
        # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user
        # retweeted_status.full_text - full text of the tweet
        # tweet.entities['hashtags'] - hashtags in the tweet# Begin scraping the tweets individually:
        noTweets = 0

        for tweet in tweet_list:# Pull the values
            location = tweet.user.location
            hashtags = tweet.entities['hashtags']
            tweetcreatedts = tweet.created_at
            try:
                text = tweet.retweeted_status.full_text
            except AttributeError:  # Not a Retweet
                text = tweet.full_text 
                # Add the 11 variables to the empty list - ith_tweet:
            if 'san francisco' in location.lower():
                ith_tweet = [location, text, hashtags, tweetcreatedts]
                db_tweets.loc[len(db_tweets)] = ith_tweet
                # increase counter - noTweets  
                noTweets += 1
            elif 'new york' in location.lower():
                ith_tweet = [location, text, hashtags, tweetcreatedts]
                db_tweets.loc[len(db_tweets)] = ith_tweet
                # increase counter - noTweets  
                noTweets += 1
            elif 'los angeles' in location.lower():
                ith_tweet = [location, text, hashtags, tweetcreatedts]
                db_tweets.loc[len(db_tweets)] = ith_tweet
                # increase counter - noTweets  
                noTweets += 1
            elif 'miami' in location.lower():
                ith_tweet = [location, text, hashtags, tweetcreatedts]
                db_tweets.loc[len(db_tweets)] = ith_tweet
                # increase counter - noTweets  
                noTweets += 1
            elif 'chicago' in location.lower():
                ith_tweet = [location, text, hashtags, tweetcreatedts]
                db_tweets.loc[len(db_tweets)] = ith_tweet
                # increase counter - noTweets  
                noTweets += 1
            else:
                pass
            # Append to dataframe - db_tweets
           
        # Run ended:
        end_run = time.time()
        duration_run = round((end_run-start_run)/60, 2)
        
        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))
        print('time take for {} run to complete is {} mins'.format(i+1, duration_run))
        if numRuns == 1:
            pass
        else:
            for i in range(15):
                time.sleep(61) #15 minute sleep time
                print(str(i+1) + " minutes of wait time passed.")

    # Once all runs have completed, save them to a single csv file:       
    from datetime import datetime
    # Obtain timestamp in a readable format
    to_csv_timestamp = datetime.today().strftime('%Y%m%d')# Define working path and filename
    path = os.getcwd()
    filename = path + '/data/' + to_csv_timestamp + '_selective_cities_covid_tweets.csv'# Store dataframe in csv with creation date timestamp
    db_tweets.to_csv(filename, index = False)
    
    program_end = time.time()
    print('Scraping has completed!')
    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))
```

```{python eval=FALSE}
## Setting the range of tweets and number of total tweets to be scraped
date_since = "2020-03-17"
numTweets = 2500
numRuns = 20# Call the function scraptweets
scraptweets(search_words, date_since, numTweets, numRuns)
```

\newpage

### Setting up in RStudio
#### Load Required Packages
We first load required packages to the notebook:
```{r message=FALSE, warning=FALSE}
library(lubridate)
library(ggplot2)
library(dplyr)
library(readr)
library(tidyverse)
library(knitr)
library(kableExtra)
```

#### Load the Data
Load the dataset scraped from twitter on Oct 5 to Oct 

```{r warning=FALSE, message=FALSE}
#For Local Version:

###### Week 1 ######
data_20201005 <- read.csv("data/20201005_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE, 
                          encoding="UTF-8") #Monday
data_20201006 <- read.csv("data/20201006_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Tuesday
data_20201007 <- read.csv("data/20201007_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Wednesday
data_20201008 <- read.csv("data/20201008_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Thursday
data_20201009 <- read.csv("data/20201009_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Friday
data_20201010 <- read.csv("data/20201010_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Saturday
data_20201011 <- read.csv("data/20201011_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Sunday

### Week2 ###
data_20201012 <- read.csv("data/20201012_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Monday
data_20201013 <- read.csv("data/20201013_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Tuesday
data_20201014 <- read.csv("data/20201014_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Wednesday
data_20201015 <- read.csv("data/20201015_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Thursday
data_20201016 <- read.csv("data/20201016_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Friday
data_20201017 <- read.csv("data/20201017_selective_cities_covid_tweets.csv", 
                          stringsAsFactors = FALSE,
                          encoding="UTF-8") #Saturday

#Combine the data for better analysis and reduce coding.
data <- bind_rows(data_20201005, data_20201006, data_20201007, 
                  data_20201008, data_20201009, data_20201010,
                  data_20201011, data_20201012, data_20201013,
                  data_20201014, data_20201015, data_20201016,
                  data_20201017)

rm(data_20201005, data_20201006, data_20201007, 
                  data_20201008, data_20201009, data_20201010,
                  data_20201011, data_20201012, data_20201013,
                  data_20201014, data_20201015, data_20201016,
   data_20201017) #release RAM usage
```

Now we have a total number of tweets:
```{r}
nrow(data)
write.csv(data, "complete_covid_tweet.csv")
```

#### Check the Loaded Data
Lets see what will the first few Data will look like after direct scrap from Twitter.
```{r message=FALSE, warning=FALSE, paged.print=TRUE}
head(data, 3)
tail(data, 3)
```

In a nice format to display the first four rows of the data set:
```{r eval=FALSE}
kable(data[1:4,], 
                format="latex", 
                booktabs=TRUE, 
                caption = "Raw Tweet Data on Oct 5 2020") %>%
  column_spec(2, width="30em") %>%
  column_spec(3, width="10em") %>%
  kable_styling(latex_options=c("scale_down", "HOLD_position"))

```


As we looking the head() and tail() of the dataset, we found that there are four main problem with the main text that we have to perform data cleaning before doing any data analysis.

\begin{itemize}
  \item Geo-location tag because it is user-defined. 
  \item Non-ASCII characters, like emoji or other icons appeared in the tweet. 
  \item HTML Tags like line break etc. But when knit with package kable and kableExtra, the line breaks and some of the non-ASCII characters are omitted. 
  \item Hyperlink
\end{itemize}  

Hence to obtain a more accurate result, we have to first perform data cleaning and wrangling so that we can perform further analysis.

\newpage

### Data Cleaning

#### Data Cleaning on Location

We first perform some data cleaning on the location name so that to make it consistent. From the data scraping stage, I only selected five cities, and they are San Francisco, New York, Los Angeles, Miami and Chicago. Hence in the code below we should only focus on these five cities.

First we write a function for shorter code.
```{r}
### Location Cleaning Function:
location_cleaning_function <- function(df1){
  count <- 1
  for (i in df1$location){
    if (grepl("San Francisco", i, ignore.case=TRUE) == TRUE)
    {
      location_text <- "San Francisco"
    }
    else if (grepl("New York", i, ignore.case=TRUE) == TRUE)
    {
      location_text <- "New York"
    }
    else if (grepl("Los Angeles", i, ignore.case=TRUE) == TRUE)
    {
      location_text <- "Los Angeles"
    }
    else if (grepl("Miami", i, ignore.case=TRUE) == TRUE)
    {
      location_text <- "Miami"
    }
    else if (grepl("Chicago", i, ignore.case=TRUE) == TRUE)
    {
      location_text <- "Chicago"
    }
    else
    {
      location_text <- i
    }
    df1$location[count] <- location_text
    count = count + 1
  }
  return(df1)
}

```

Checking to see if the location is in correct format:
```{r}
data_1 <- location_cleaning_function(data)
# First Data Cleaning

kable(data_1[1:4,], 
      format="latex", 
      booktabs=TRUE,
      caption = "Oct 5 2020 Tweet with Location Filtered") %>%
  column_spec(2, width="30em") %>%
  kable_styling(latex_options=c("scale_down", "HOLD_position"))
```


##### Data Cleaning on Main Tweet Text

We found that in the main tweet text, there are some characters are meaningless to human, for example line breaks, and also non-UTF-8 charcters. Also, we found in most of the tweets, there is a hyperlink to a person's tweet. Since it is not text generated by user, and we should remove it before doing any analysis.

```{r}
main_text_clean <- function(df2){
  count <- 1
  for (i in df2$text) {
    tweet_text <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", i) #Remove Hyperlink
    tweet_text <- gsub("[\n]", " ", tweet_text) #removing html and css codes within tweets.
    #tweet_text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "", tweet_text) #Remove non-ASCII
    #tweet_text <- gsub("U00..", "", tweet_text) #Remove Emoji
    #print(tweet_text)
    df2$text[count] <- tweet_text
    count = count + 1
  }
  return(df2)
}

```

```{r}
data_1a <- main_text_clean(data_1)
# Main Text Cleaning
```

```{r}
kable(data_1a[1:4,], 
      format="latex", 
      booktabs=TRUE,
      caption = "Oct 5 2020 Tweet with Location @ Text Filtered") %>%
  column_spec(2, width="30em") %>%
  kable_styling(latex_options=c("scale_down", "HOLD_position"))
```

##### Creating a column for a tweet mentioning other Twitter User:
```{r}

```

##### Data Cleaning on Time

Since we only interested in the date of the tweets that is created, we can convert the time with hour, minute, second into date only.
```{r}
data_1c <- data_1a %>%
  mutate(tweetcreatedts = as.Date(tweetcreatedts))

tail(data_1c)
```

##### Data Checking
Checking the output of the dataframe after cleaning the text:


\newpage
### Basic Analysis on Tweets

#### Summary Statistics on Tweet Text

Since in the dataset there are only five cities selected, we first look into the distribution of cities.
```{r message=FALSE, warning=FALSE}
library(maps)

population_data <- world.cities %>%
  filter(name %in% c("Chicago", "Los Angeles", "Miami", "New York", "San Francisco"),
         country.etc == "USA") %>%
    select(name, pop)
  
data_stat1 <- data_1c %>%
  group_by(location) %>%
  summarise(
  number_of_tweet=n(),
  tweet_ratio = n()/nrow(data_1c)
  ) %>%
  inner_join(population_data, by=c("location" = "name")) %>%
  mutate(pop_ratio = pop/sum(pop))
  
table_1 <- kable(data_stat1,
                 booktabs=TRUE,
                 caption = "Summary Statistics of Tweets")  %>%
    kable_styling(latex_options="HOLD_position")
  
table_1
```

If we visualize the data in plots,
```{r message=FALSE, warning=FALSE}
data_plot1 <- data_stat1 %>%
    select(location, pop_ratio, tweet_ratio) %>%
    gather(Total, Value, -location)
  
plot_1 <- ggplot(data_plot1, aes(x = location, y=Value, fill = Total)) +
    geom_col(position = "dodge") +
    labs(title="Chart of Tweet Ratio and Population Ratio in the 5 Cities")
  
plot_1
```

In the graph above, we find that the tweet ratio and the population ratio of the five cities are similar, so that we can continue on analyzing the tweet as the portions of tweets of the dataset is similar to the real population ratio, meaning that it should be a good sampling when we have around 600 extracted tweets with specific locations.


Average Number of Characters in the Tweet:
```{r message=FALSE, warning=FALSE}
round(mean(nchar(data_1c$text)),0)
```
Since characters can not be in fractions, so we going to round up the number of characters.
If we look into cities statistics:

```{r warning=FALSE, message=FALSE}
data_stat2 <- data_1c %>%
  group_by(location) %>%
  summarise(
    number_of_tweets = n(),
    Avg_character_in_tweets = round(mean(nchar(text)),0),
    character_limit_ratio = Avg_character_in_tweets/280
  )

kable(data_stat2,
                 booktabs=TRUE,
                 caption = "Summary Statistics of Tweets")  %>%
    kable_styling(latex_options="HOLD_position")
```

If we want to look briefly what tweets are about across the USA, the top 10 words appeared in the tweet,

```{r warning=FALSE, message=FALSE}
library(tidytext)
text_df <- tibble(tweet = 1:nrow(data_1c), text=data_1c$text)
text_df_table <- text_df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, sort=TRUE) %>%
  head(., 10)

kable(text_df_table,
                 booktabs=TRUE,
                 caption = "Summary Statistics of Tweets")  %>%
    kable_styling(latex_options= "HOLD_position")
```

Interestingly, "Trump" appeared third most popular among tweets about COVID-19. But if we only look into single words its not helpful in seeing the overall picture of the sentiment analysis. Hence in the next part, we are going to use a tidy approach to analyze the tweets and hopefully get some insights over the weeks.

```{r}
#get_sentiments("afinn")
  
```


\newpage
### Further Analysis on relationship between Tweets and COVID-19 Cases.

In here, I will extract COVID-19 daily cases from Johns Hopkins Coronavirus Resource Center to analyze the tweets and the sentiment.

In New York City, it consist of five counties, and they are:

\begin{itemize}
  \item Bronx County with population approximately 1.4 million 
  \item Kings County with population approximately 2.5 million
  \item New York County with population approximately 1.6 millon
  \item Queens County with population approximately 2.2 million
  \item Richmond County with population approximately 500 thousands.
\end{itemize}

The total population is around 8.2 million which is similar to the result pulled from the package maps and the result above. Hence, to obtain New York City's COVID-19 data, I will consider the sum of these 5 counties.

\begin{itemize}
\item For Miami, I will use the Miami-Dade County, Florida data.
\item For Los Angeles, I will use the Los Angeles County, California data.
\item For San Francisco, I will use the San Francisco County, California data.
\item For Chicago, I will use Cook County, Illinois data.
\item For New York City, I will use the combined 5 counties data
\end{itemize}

Since as I search I can't find a complete dataset for all the counties, I will use separate state's open data source to obtain the COVID-19 Cases and related figures. 
\begin{itemize}
\item For State of California, I used the data set on https://data.ca.gov and the URL is :https://data.ca.gov/dataset/covid-19-cases/resource/926fd08f-cc91-4828-af38-bd45de97f8c3
\item For New York City, I used the data set published by NYC Health department on GitHub, and the URL is: https://github.com/nychealth/coronavirus-data. There is a downside as in the readme file on GitHub stated, *Note that sum of counts in this file may not match values in Citywide tables because of records with missing geographic information. This file does not contain information on probable deaths.*
\end{itemize}

#### Data Download:

```{r}
california_covid <- read.csv("https://data.ca.gov/dataset/590188d5-8545-4c93-a9a0-e230f0db7290/resource/926fd08f-cc91-4828-af38-bd45de97f8c3/download/statewide_cases.csv", encoding="UTF-8")
```

```{r}
SF_covid <- california_covid %>%
  filter(county=="San Francisco") %>%
  mutate(date = as.Date(date)) %>%
  filter(date >= as.Date("2020-10-05")) %>%
  filter(date <= as.Date("2020-10-17")) %>%
  mutate(SF_case_per_100k = totalcountconfirmed/723724*100000) %>%
  select(date, SF_case_per_100k)

LA_covid <- california_covid %>%
  filter(county=="Los Angeles") %>%
  mutate(date = as.Date(date)) %>%
  filter(date >= as.Date("2020-10-05"))%>%
  filter(date <= as.Date("2020-10-17"))%>%
  mutate(LA_case_per_100k = totalcountconfirmed/3911500*100000)%>%
  select(date, LA_case_per_100k)


```

```{r}
SF_covid
```

```{r}
LA_covid
```

```{r}
NYC_covid_raw <- read.csv("https://raw.github.com/nychealth/coronavirus-data/master/boro/boroughs-case-hosp-death.csv")
```

Since the structure of the GitHub version of data set is not the the desired data structure, we have to perform some data cleaning.

```{r}
NYC_covid <- NYC_covid_raw %>%
  mutate(date = as.Date(DATE_OF_INTEREST, format="%m/%d/%Y")) %>%
  mutate(Brooklyn = cumsum(BK_CASE_COUNT), 
         Bronx = cumsum(BX_CASE_COUNT), 
         Manhattan = cumsum(MN_CASE_COUNT), 
         Queens = cumsum(QN_CASE_COUNT), 
         Staten_Island = cumsum(SI_CASE_COUNT)) %>%
  filter(date >= as.Date("2020-10-05")) %>%
  filter(date <= as.Date("2020-10-17")) %>%
  select(date, Brooklyn, Bronx, Manhattan, Queens, Staten_Island) %>%
  rowwise() %>%
  mutate(total = sum(c_across(Brooklyn:Staten_Island))) %>%
  mutate(NYC_case_per_100k = total/8124427*100000) %>%
  select(date, NYC_case_per_100k)

kable(NYC_covid, 
      booktabs=TRUE,
      caption = "COVID-19 Daily Cases by County") %>%
  kable_styling(latex_options= "HOLD_position")
```
Trends of COVID-19 Cases in 5 Cities
```{r}
covid_plot <- NYC_covid %>%
  inner_join(SF_covid, by=c("date"="date")) %>%
  inner_join(LA_covid, by=c("date"="date"))

covid_plot
```

```{r}
ggplot(covid_plot, aes(x=date)) +
  geom_line(aes(y=NYC_case_per_100k, color="blue")) +
  geom_line(aes(y=LA_case_per_100k, color="green")) +
  geom_line(aes(y=SF_case_per_100k, color="red"))  +
  scale_color_discrete(name = "Cities", labels = c("NYC", "LA", "SF")) +
  labs(title="Reported Cases per 100K Population in 5 selected cities") +
  xlab("Date") +
  ylab("Reported Cases Per 100K Population") +
  scale_y_continuous(trans = "log")
```

#### Basic Data Exploration:
Compare the actual number of cases for each county is not that useful because some counties have a population more than a few millions but some counties only have a few thousands. Hence if we would like to compare the seriousness of the counties data, we should look into the case rate, which we can present in cases per 100,000 populations, or cases per 10,000 populations. In this analysis, we can analyze using the rate of cases per 100,000 populations as all the cities I listed here are having population more than 100,000.
\newline





